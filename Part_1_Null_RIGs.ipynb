{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Null RIGs using synthetic data (the features are given)\n",
    "## Estimating the number of overfit features for a given pipeline\n",
    "* Can include the p-value confidence intervals\n",
    "* Treat empirically derived features as if they were pre-specificed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sparkbeyond._api2.classes as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# For my API token\n",
    "import os\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed class from the sampling_toolbox\n",
    "from sampling_toolbox import PermutationObject, operational_log_number_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic\n",
    "We create synthetic data for titanic based on the number of rows of the training data and the support of the minority class in the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 ms, sys: 5.55 ms, total: 20.9 ms\n",
      "Wall time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "titanic = PermutationObject()\n",
    "\n",
    "titanic.set_data_parameters(nrows = 720, minority_class = 0.37)\n",
    "\n",
    "titanic.create_synthetic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run for 1000 permutations\n",
    "We will run for now and look into why 1000 was chosen later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max null RIG \t\t= 0.012108454756697327\n",
      "Median null RIG \t= 0.0004783449261842156\n",
      "1 in 1000 null RIG \t= 0.00876674555343226\n",
      "Gain threshold \t\t= 0.0005\n",
      "CPU times: user 12.3 s, sys: 63.7 ms, total: 12.3 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "titanic.calculate_null_rigs(permutations = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441 in 1000 null RIGs greater than 0.0005 threshold\n"
     ]
    }
   ],
   "source": [
    "titanic.null_rigs_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 in 1000 null RIGs greater than 0.01 threshold\n"
     ]
    }
   ],
   "source": [
    "titanic.gain_threshold = 0.01\n",
    "titanic.null_rigs_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run for a larger number of permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max null RIG \t\t= 0.018037440508354128\n",
      "Median null RIG \t= 0.0004783449261842156\n",
      "1 in 10000 null RIG \t= 0.016612823855212518\n",
      "Gain threshold \t\t= 0.01\n",
      "CPU times: user 1min 58s, sys: 706 ms, total: 1min 58s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "titanic.calculate_null_rigs(permutations = 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 in 10000 null RIGs greater than 0.01 threshold\n"
     ]
    }
   ],
   "source": [
    "titanic.null_rigs_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 in 10000\n"
     ]
    }
   ],
   "source": [
    "titanic.gain_threshold = 0.02\n",
    "titanic.null_rigs_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Titanic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to https://demo.sparkbeyond.com/...\n",
      "Connected. Server version 1.36.1\n"
     ]
    }
   ],
   "source": [
    "#api_key = os.environ['SB_Demo_API_key']\n",
    "api_key = 'eyJhbGciOiJIUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI3YzE2YmVjYy1lZjQ0LTQ2NjYtOGI4Ni0xMDVmZWQ3ZGVlOTkifQ.eyJpYXQiOjE2NDc4OTIxMTAsImp0aSI6ImI4YzEyMTA0LThhMDUtNDJlYi04NDNlLWZjMWRhMzkwODU3MCIsImlzcyI6Imh0dHBzOi8vZGVtby5zcGFya2JleW9uZC5jb20vYXV0aC9yZWFsbXMvc3BhcmtiZXlvbmQiLCJhdWQiOiJodHRwczovL2RlbW8uc3BhcmtiZXlvbmQuY29tL2F1dGgvcmVhbG1zL3NwYXJrYmV5b25kIiwic3ViIjoiMjYyMzc0NzEtYjIyZS00YzUxLWFiZjItZDBkOGI2NGI3YzFiIiwidHlwIjoiT2ZmbGluZSIsImF6cCI6ImRpc2NvdmVyeS1zZGsiLCJzZXNzaW9uX3N0YXRlIjoiZDA1ODk0M2UtNmRmZS00ODFhLWEwZDItMmQ1ODc3ZTcyYTM0Iiwic2NvcGUiOiJwcm9maWxlIGVtYWlsIG9mZmxpbmVfYWNjZXNzIn0.b-GO6G-fR69jBcj9PGK74gcDqeM5nueAv0fviQ7RVFg'\n",
    "server_url = 'https://demo.sparkbeyond.com/'\n",
    "client = sb.SparkBeyondClient(base_url=server_url, api_key=api_key, verify_ssl_certificate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way to calculating null RIGs, I ran a pipeline with a shuffled target to observe features generated by Learn for a single null RIG permutation. By definition the features generated should be \"null features\" that include the process of feature search and feature ranking. \n",
    "\n",
    "Then using a gain threshold set to the 1/1000 null RIG from before, we can see how many features have RIG values above this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LearningSettings(\n",
       "    problem_definition=ProblemDefinition(target_column=ColumnParam(value='survived')),\n",
       "    feature_generator_settings=FeatureGenerationSettings(gain_threshold=0.0084),\n",
       "    feature_count=[1000]\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = client.revision(project_name = 'Titanic_-_Survival_Prediction_peter', revision_id=30)\n",
    "model.learning_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>feature</th>\n",
       "      <th>Input names</th>\n",
       "      <th>Dominant survived</th>\n",
       "      <th>RIG</th>\n",
       "      <th>Score</th>\n",
       "      <th>lin. score</th>\n",
       "      <th>Support 0</th>\n",
       "      <th>Support 1</th>\n",
       "      <th>% support 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Median</th>\n",
       "      <th>75th Percentile</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "      <th>Estimated Total Values</th>\n",
       "      <th>Summary is Sampled</th>\n",
       "      <th>numericFeatureName</th>\n",
       "      <th>booleanFeatureName</th>\n",
       "      <th>revision_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>numericPairs(ticket) for (c.a.) &gt;= 33111.5</td>\n",
       "      <td>ticket</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011639</td>\n",
       "      <td>0.011509</td>\n",
       "      <td>0.623454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>...</td>\n",
       "      <td>1.872300e+04</td>\n",
       "      <td>3.192100e+04</td>\n",
       "      <td>3.767100e+04</td>\n",
       "      <td>1.729200e+04</td>\n",
       "      <td>1.464800e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>numericPairs(ticket) for (c.a.)</td>\n",
       "      <td>numericPairs(ticket) for (c.a.) &gt;= 33111.5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>alphanumericRatio(name) &gt;= 0.851</td>\n",
       "      <td>name</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.538697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>...</td>\n",
       "      <td>9.223372e-11</td>\n",
       "      <td>9.223372e-11</td>\n",
       "      <td>9.223372e-11</td>\n",
       "      <td>9.223372e-11</td>\n",
       "      <td>9.223372e-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>alphanumericRatio(name)</td>\n",
       "      <td>alphanumericRatio(name) &gt;= 0.851</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>slidingSubsets(name) contains (\"mr\", \"william\")</td>\n",
       "      <td>name</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010055</td>\n",
       "      <td>0.009613</td>\n",
       "      <td>0.535619</td>\n",
       "      <td>28.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.42%</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7567.0</td>\n",
       "      <td>False</td>\n",
       "      <td>slidingSubsets(name) contains (\"mr\", \"william\")</td>\n",
       "      <td>slidingSubsets(name) contains (\"mr\", \"william\")</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                          feature Input names  \\\n",
       "0    0       numericPairs(ticket) for (c.a.) >= 33111.5      ticket   \n",
       "1    1                 alphanumericRatio(name) >= 0.851        name   \n",
       "2    2  slidingSubsets(name) contains (\"mr\", \"william\")        name   \n",
       "\n",
       "   Dominant survived       RIG     Score  lin. score  Support 0  Support 1  \\\n",
       "0                  1  0.011639  0.011509    0.623454        0.0        6.0   \n",
       "1                  1  0.009688  0.009616    0.538697        0.0        5.0   \n",
       "2                  0  0.010055  0.009613    0.535619       28.0        5.0   \n",
       "\n",
       "  % support 0  ...        Median  75th Percentile           Max          Mean  \\\n",
       "0        0.0%  ...  1.872300e+04     3.192100e+04  3.767100e+04  1.729200e+04   \n",
       "1        0.0%  ...  9.223372e-11     9.223372e-11  9.223372e-11  9.223372e-11   \n",
       "2       6.42%  ...           NaN              NaN           NaN           NaN   \n",
       "\n",
       "             SD  Estimated Total Values Summary is Sampled  \\\n",
       "0  1.464800e+04                     NaN              False   \n",
       "1  9.223372e-11                     NaN              False   \n",
       "2           NaN                  7567.0              False   \n",
       "\n",
       "                                numericFeatureName  \\\n",
       "0                  numericPairs(ticket) for (c.a.)   \n",
       "1                          alphanumericRatio(name)   \n",
       "2  slidingSubsets(name) contains (\"mr\", \"william\")   \n",
       "\n",
       "                                booleanFeatureName  revision_id  \n",
       "0       numericPairs(ticket) for (c.a.) >= 33111.5           30  \n",
       "1                 alphanumericRatio(name) >= 0.851           30  \n",
       "2  slidingSubsets(name) contains (\"mr\", \"william\")           30  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = model.features()\n",
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the number of features evaluated\n",
    "I estimated the number of features generated using the operational log generated from learn for a pipeline run on the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = \"\"\"19:29:27 - Learning to classify survived (revision #25). Evaluating using AUC\n",
    "19:29:32 - Best feature (RIG: 0.0031) of 2 from parch is: parch == 4\n",
    "19:29:32 - Best feature (RIG: 0.0036) of 3 from sibsp is: sibsp == 5\n",
    "19:29:32 - Best feature (RIG: 0.005) of 2 from fare is: ceil(fare) notInRange (7.5 to 255.5)\n",
    "19:29:32 - Best feature (RIG: 0.0079) of 3 from fare is: inverse(fare) inRange (0.13 to 0.14)\n",
    "19:29:32 - Best feature (RIG: 0.0079) of 3 from fare is: log1p(fare) inRange (2.11 to 2.17)\n",
    "19:29:32 - Best feature (RIG: 0.005) of 2 from fare is: floor(fare) notInRange (6.5 to 254.5)\n",
    "19:29:32 - Best feature (RIG: 0.0058) of 15 from cabin is: extractKeys(elementCount(cabin)) == ('8', 'C', '6')\n",
    "19:29:32 - Best feature (RIG: 0.0058) of 18 from cabin is: splitDigitsAndLetters(cabin) contains \"68\"\n",
    "19:29:32 - Best feature (RIG: 0.0116) of 34 from ticket is: numericPairs(ticket) for (c.a.) >= 33111.5\n",
    "19:29:32 - Best feature (RIG: 0.0087) of 46 from ticket is: min(numbers(ticket)) inRange (34,239 to 36,716)\n",
    "19:29:32 - Best feature (RIG: 0.0084) of 34 from cabin is: prefixes(cabin) contains \"C9\"\n",
    "19:29:32 - Best feature (RIG: 0.0116) of 33 from ticket is: numericPairsUSFormat(ticket) for (c.a.) >= 33111.5\n",
    "19:29:32 - Best feature (RIG: 0.0087) of 239 from ticket is: sum(numbers(ticket)) inRange (34,239 to 36,716)\n",
    "19:29:32 - Best feature (RIG: 0.0097) of 194 from name is: alphanumericRatio(name) >= 0.851\n",
    "19:29:32 - Best feature (RIG: 0.0101) of 60 from name is: slidingSubsets(name) contains (\"mr\", \"william\")\n",
    "19:29:32 - Best feature (RIG: 0.0073) of 69 from cabin is: The percent of 6 in (cabin) inRange (17.14 to 26.79)\n",
    "19:29:32 - Best feature (RIG: 0.0077) of 77 from name is: stemmedWords(name) contains \"katherin\"\n",
    "19:29:32 - Best feature (RIG: 0.0091) of 131 from name is: name contains \"mr. william\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "965"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operational_log_number_of_features(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>feature_0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  feature_0.1\n",
       "0         0            0\n",
       "1         1            0\n",
       "2         0            0\n",
       "3         0            0\n",
       "4         0            0\n",
       "..      ...          ...\n",
       "715       0            0\n",
       "716       1            1\n",
       "717       0            0\n",
       "718       1            0\n",
       "719       0            0\n",
       "\n",
       "[720 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x: float):\n",
    "    \"\"\"Function for returning entropy\"\"\"\n",
    "    if (x == 0) | (x == 1):\n",
    "        return 0\n",
    "    else:\n",
    "        h = (-x*np.log2(x)) - ((1-x)*np.log2(1-x))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.36944444444444446, 0.9502445670610749)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.data['target'].mean() , entropy(titanic.data['target'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = titanic.data['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9502445670610749"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = titanic.data.copy()\n",
    "df.head()\n",
    "target = 'target'\n",
    "feature = 'feature_0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_0.1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.628086</td>\n",
       "      <td>0.371914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.347222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target              0         1\n",
       "feature_0.1                    \n",
       "0            0.628086  0.371914\n",
       "1            0.652778  0.347222"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pd.crosstab(df[feature], df[target], normalize='index')\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 643 µs, sys: 27 µs, total: 670 µs\n",
      "Wall time: 677 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature_0.1\n",
       "0    0.952130\n",
       "1    0.931563\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "s_entropy = counts.loc[:, 1].apply(entropy)\n",
    "s_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 ms, sys: 48 µs, total: 1.32 ms\n",
      "Wall time: 1.33 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature_0.1\n",
       "0    0.952130\n",
       "1    0.931563\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "s_entropy = counts.apply(entropy_new, axis=1)\n",
    "s_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 ms, sys: 1.7 ms, total: 16.3 ms\n",
      "Wall time: 15 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature_0.1\n",
       "0    0.952130\n",
       "1    0.931563\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "s_entropy = pd.crosstab(df[feature], df[target], normalize='index').loc[:, 1].apply(entropy)\n",
    "s_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_prior = entropy(df[target].mean())\n",
    "\n",
    "# Probability feature is true\n",
    "probability_feature_true = df[feature].mean()\n",
    "\n",
    "# Entropy of target given feature\n",
    "s_entropy = pd.crosstab(df[feature], df[target], normalize='index').loc[:, 1].apply(entropy)\n",
    "\n",
    "# If the feature only has one outcome\n",
    "if len(s_entropy) < 2:\n",
    "    if s_entropy.index[0] == 0:\n",
    "        h_feature_f = s_entropy[0]*(1-probability_feature_true)\n",
    "        h_feature_t = 0\n",
    "    else:\n",
    "        h_feature_f = 0\n",
    "        h_feature_t = s_entropy[1]*probability_feature_true\n",
    "\n",
    "else:\n",
    "    h_feature_t = s_entropy[1]*probability_feature_true\n",
    "    h_feature_f = s_entropy[0]*(1-probability_feature_true)\n",
    "\n",
    "return (h_prior - (h_feature_t + h_feature_f))/h_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target column\n",
    "target = 'price'\n",
    "\n",
    "# Previously run pipeline\n",
    "proj_name = 'House_Sales_in_King_County_-_Regression_with_contexts_peter'\n",
    "revision = 16\n",
    "\n",
    "# File on which feature search has been run with a explicit partition column (`splitColumn`)\n",
    "filepath = '../../../Data/KingCounty/KingCountyHouseSales_with_random_parition_column.csv.gz'\n",
    "\n",
    "# Number of bootstrap resamples\n",
    "resamples = 10\n",
    "\n",
    "bins = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = client.revision(proj_name, revision_id=revision)\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_files(df,proj,target):\n",
    "    \n",
    "    # Get the features\n",
    "    df_features = proj.features()\n",
    "    number_of_features = df_features.shape[0]\n",
    "    \n",
    "    ######## NEED TO CHANGE THIS FOR EACH PROJECT IF NON-DEFAULT VALUES ARE USED #########\n",
    "    \n",
    "    # Change the default settings to match those from the UI\n",
    "    train = client.upload_dataframe(df[df.splitColumn == 'Train'], \n",
    "                                    project_name = proj.project_name,\n",
    "                                    target_path='train.tsv.gz',\n",
    "                                    overwrite = True)\n",
    "    train.source.settings.format.use_escaping = True\n",
    "\n",
    "    test = client.upload_dataframe(df[df.splitColumn == 'Test'], \n",
    "                                   project_name = proj.project_name,\n",
    "                                   target_path = 'test.tsv.gz', \n",
    "                                   overwrite = True)\n",
    "    test.source.settings.format.use_escaping = True\n",
    "\n",
    "    ######################################################################################\n",
    "    # Enriched the Train data\n",
    "    \n",
    "    Enrich_params = sb.EnrichParams(inputs = [train], \n",
    "                                    include_originals = True,\n",
    "                                    enforce_boolean_numeric = True)\n",
    "    \n",
    "    df_enriched_train = proj.enrich(Enrich_params).results_dataframe()\n",
    "\n",
    "    # Enrich the Test data\n",
    "    Enrich_params = sb.EnrichParams(inputs = [test], \n",
    "                                    include_originals = True,\n",
    "                                    enforce_boolean_numeric = True)\n",
    "    \n",
    "    df_enriched_test = proj.enrich(Enrich_params).results_dataframe()\n",
    "\n",
    "    # Clean the enriched data to get just the features\n",
    "    df_train_clean = df_enriched_train[df_enriched_train.columns[-number_of_features:].tolist() + [target]]\n",
    "    df_test_clean = df_enriched_test[df_enriched_test.columns[-number_of_features:].tolist() + [target]]\n",
    "    \n",
    "    return {'df_features' : df_features, 'df_train_clean': df_train_clean, 'df_test_clean' : df_test_clean}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bee457483f4d649366b749c6ca9858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Hashing dataframe contents', max=17252, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target filename with dataframe hash is train-c4248339c0d5dce86bd69ff5f392ec26.tsv.gz\n",
      "Writing dataframe to temp file /var/folders/z4/ymtx_b595ngcqjv1bwwv9qjh0000gn/T/trainkknfnmn2.tsv.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89db02e99e8e44e9bd28936aa0a9fe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Uploading', max=492481, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f9b8016f514658885d2cf4ac8c2065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Hashing dataframe contents', max=4358, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target filename with dataframe hash is test-6bbd5d8cbf2678a1397a639f2bd89ac8.tsv.gz\n",
      "Writing dataframe to temp file /var/folders/z4/ymtx_b595ngcqjv1bwwv9qjh0000gn/T/testlp1tkz8c.tsv.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3efde95b6634fbca55f921f47aea03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Uploading', max=127387, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job running, started at 2022-03-23 18:39:43.461000\n",
      "Finished building new contexts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75d5f835da44f7c9d03af1870c7673f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Rows processed', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job completed: EnrichPredictJobResult(report_filenames=[])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4272ec3641764b7bb813127248a24511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Downloading to /var/folders/z4/ymtx_b595ngc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job running, started at 2022-03-23 18:40:19.721000\n",
      "Finished building new contexts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1cff02991e40c9a1d84dddbc661f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Rows processed', max=1, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job completed: EnrichPredictJobResult(report_filenames=[])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bffd6f5e144c33905fdb54ecc20ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Downloading to /var/folders/z4/ymtx_b595ngc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Enrich the data and get the required dataframes\n",
    "pipeline_dict = get_pipeline_files(df,proj,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of the dataframes\n",
    "df_train_clean = pipeline_dict['df_train_clean'].copy()\n",
    "df_test_clean = pipeline_dict['df_test_clean'].copy()\n",
    "df_features = pipeline_dict['df_features'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_regression_bins(df: pd.core.frame.DataFrame):\n",
    "    '''Function to get the regression bins from a DP feature dataframe\n",
    "    \n",
    "    Inputs\n",
    "    ----------\n",
    "    df    - SparkBeyond features dataframe (client.revision().features())\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    bins  - A list of the bin values '''\n",
    "    \n",
    "    # Extract the lift columns\n",
    "    columns = df.columns[df.columns.str.contains('Lift')]\n",
    "    \n",
    "    # Take the first number from each column\n",
    "    bins = [int(re.search(pattern=\"\\d+\", string=column).group()) for column in columns\n",
    "                                 if re.search(pattern=\"\\d+\", string=column)]\n",
    "\n",
    "    # Append the last number from the last column\n",
    "    bins.append(int(re.search(string = columns[-1], pattern = \"\\d+$\").group()))\n",
    "    \n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude(latLong) < 47.531\n",
      "[75000, 278000, 360000, 450000, 565500, 760000, 5570000]\n"
     ]
    }
   ],
   "source": [
    "feature =  df_train_clean.columns[0]\n",
    "print(feature)\n",
    "\n",
    "binary_target = df[target].nunique() == 2\n",
    "\n",
    "if binary_target:\n",
    "    # check RIG calculation for first feature (slight discrepancy - will need to check)\n",
    "    Y_train = df_train_clean[target]\n",
    "    Y_test = df_test_clean[target]\n",
    "\n",
    "else:\n",
    "    bins = get_regression_bins(df_features)\n",
    "    print(bins)\n",
    "    Y_train = pd.cut(df_train_clean[target], bins = bins)\n",
    "    Y_test = pd.cut(df_test_clean[target], bins = bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SB Train RIG \t\t= 0.11051776976058203\n"
     ]
    }
   ],
   "source": [
    "mask = df_features.feature == feature\n",
    "print('SB Train RIG \\t\\t= {}'.format(df_features.loc[mask, 'RIG'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    0.0\n",
       "Name: latitude(latLong) < 47.531, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_clean[feature].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (760000, 5570000]\n",
       "1    (760000, 5570000]\n",
       "2    (760000, 5570000]\n",
       "3    (760000, 5570000]\n",
       "4    (760000, 5570000]\n",
       "Name: price, dtype: category\n",
       "Categories (6, interval[int64]): [(75000, 278000] < (278000, 360000] < (360000, 450000] < (450000, 565500] < (565500, 760000] < (760000, 5570000]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entropy_new(values):\n",
    "    h = []\n",
    "    for value in values:\n",
    "        if (value == 0) | (value == 1):\n",
    "            h.append(0)\n",
    "        else:\n",
    "            h.append(-value*np.log2(value))\n",
    "    return sum(h)\n",
    "\n",
    "def rig(x, y):\n",
    "    # Prior Entropy\n",
    "    prior_counts = pd.value_counts(y, normalize=True).to_frame()\n",
    "    pe = prior_counts.apply(entropy_new).values[0]\n",
    "\n",
    "    counts = pd.crosstab(x, y, normalize='index')\n",
    "    f_weights = pd.value_counts(x, normalize=True)\n",
    "\n",
    "    f_entropy = counts.apply(entropy_new, axis=1)\n",
    "\n",
    "    # Conditional entropy\n",
    "    ce = sum(f_weights*f_entropy)\n",
    "\n",
    "    RIG = (pe-ce)/pe\n",
    "    return RIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.9 ms, sys: 3.98 ms, total: 41.8 ms\n",
      "Wall time: 39 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1104997505700234"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rig(x = df_train_clean[feature], y = Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = titanic.data.copy()\n",
    "df.head()\n",
    "target = 'target'\n",
    "feature = 'feature_0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 2.72 ms, total: 22.7 ms\n",
      "Wall time: 20.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00018019587939709127"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rig(df[feature], df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'feature_0.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/sampling_toolbox/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feature_0.1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-d1c62a0ee28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calculated Train RIG \\t= {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calculated Test RIG \\t= {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sampling_toolbox/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sampling_toolbox/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feature_0.1'"
     ]
    }
   ],
   "source": [
    "print('Calculated Train RIG \\t= {}'.format(str(rig(df_train_clean[feature],Y_train))))\n",
    "print('Calculated Test RIG \\t= {}'.format(str(rig(df_test_clean[feature], Y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
